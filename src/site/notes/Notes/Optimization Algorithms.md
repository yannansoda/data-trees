---
{"topic":"Math, MachineLearning, DeepLearning","dg-publish":true,"permalink":"/Notes/Optimization Algorithms/","dgPassFrontmatter":true,"noteIcon":""}
---

## Convex optimization
- [[_assets/images/Convexity\|Convexity]]

## Gradient descent
- [[Notes/Gradient Descent\|Gradient Descent]]

## Optimization challenges
### Common challenges
- local minima
- saddle points
	- any location where all gradients of a function vanish but which is neither a global nor a local minimum
	- ![Pasted image 20241008162502.png|200](/img/user/_assets/images/Pasted%20image%2020241008162502.png)
### Challenges in DL
- Almost all optimization problems arising in deep learning are *nonconvex*
- [[Notes/Gradient Descent#Challenges and Solutions\|Gradient Descent#Challenges and Solutions]]




